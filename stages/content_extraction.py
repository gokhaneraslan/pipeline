import logging
import textwrap
from pathlib import Path
from typing import List, Any, Optional

from processing.document_parser import get_document_content, get_semantic_splitter, split_document_into_nodes
from processing.llm_interactor import llm_extract_clean_content_from_chunk
from core.prompts import get_clean_content_extraction_prompt


logger = logging.getLogger("DataPipeline")


def final_content_extraction(
    input_files_from_stage4: List[Path],
    final_output_dir: Path,
    final_output_filename: str,
    topic: str,
    text_wrap_width: int,
    # SemanticSplitter config
    embedding_provider: str,
    ollama_embed_model: Optional[str],
    ollama_url: Optional[str],
    st_embed_model: Optional[str],
    semantic_splitter_buffer: int,
    semantic_splitter_breakpoint_perc: int,
    # LLM config for extraction
    llm_client: Any,
    llm_provider_name: str,
    llm_extraction_model_name: str,
    llm_temperature: float,
    llm_max_retries: int,
    llm_retry_delay: int,
    file_separator_text: str = "\n\n" + "="*80 + "\n\n" ) -> Optional[Path]:
    """
    Executes Stage 5: For each file from Stage 4, splits it semantically,
    uses an LLM to extract topic-focused content from each chunk,
    and combines all extracted content into a single final text file.
    """
    
    logger.info(f"--- Starting Stage 5: Final Content Extraction and Compilation ---")
    logger.info(f"Input files (from Stage 4): {len(input_files_from_stage4)}")
    logger.info(f"Final output directory: {final_output_dir.resolve()}, Filename: {final_output_filename}")
    logger.info(f"Topic for extraction: '{topic}'")

    final_output_dir.mkdir(parents=True, exist_ok=True)
    final_output_file_path = final_output_dir / final_output_filename

    if not input_files_from_stage4:
        
        logger.warning("No input files received from Stage 4. Stage 5 has nothing to process.")
        
        try:
            final_output_file_path.write_text("", encoding='utf-8')
            logger.info(f"Created empty output file at {final_output_file_path} as no input files were provided.")
            
        except Exception as e:
            logger.error(f"Failed to create empty output file {final_output_file_path}: {e}", exc_info=True)
            
        return None

    try:
        
        semantic_splitter = get_semantic_splitter(
            embedding_model_provider=embedding_provider,
            ollama_embedding_model_name=ollama_embed_model,
            ollama_base_url=ollama_url,
            sentence_transformers_model_name=st_embed_model,
            buffer_size=semantic_splitter_buffer,
            breakpoint_percentile_threshold=semantic_splitter_breakpoint_perc
        )
        
    except Exception as e:
        logger.critical(f"Failed to initialize SemanticSplitter for Stage 5: {e}. Aborting Stage 5.", exc_info=True)
        return None


    all_compiled_texts: List[str] = []

    for file_idx, file_path in enumerate(input_files_from_stage4):
        logger.info(f"\nProcessing file {file_idx + 1}/{len(input_files_from_stage4)} for content extraction: {file_path.name}")
        
        file_content = get_document_content(file_path)
        
        if file_content is None:
            logger.warning(f"Skipping file {file_path.name}: content could not be read.")
            continue
        
        if not file_content.strip():
            logger.warning(f"Skipping file {file_path.name}: content is empty.")
            continue

        text_nodes = split_document_into_nodes(file_content, file_path.name, semantic_splitter)
        if not text_nodes:
            logger.warning(f"No text nodes/chunks generated by SemanticSplitter for {file_path.name}. Skipping this file for extraction.")
            continue
        
        extracted_content_for_this_file_segments: List[str] = []
        for i, node in enumerate(text_nodes):
            chunk_text = node.get_content()
            
            if not chunk_text.strip():
                logger.debug(f"Skipping empty chunk {i+1}/{len(text_nodes)} in {file_path.name} (Extraction Stage)")
                continue

            extracted_segment = llm_extract_clean_content_from_chunk(
                llm_client=llm_client,
                provider_name=llm_provider_name,
                model_name=llm_extraction_model_name,
                topic=topic,
                chunk_text=chunk_text,
                temperature=llm_temperature,
                max_retries=llm_max_retries,
                retry_delay=llm_retry_delay,
                content_extraction_prompt_func=get_clean_content_extraction_prompt
            )
            
            if extracted_segment:
                logger.debug(f"Extracted valid segment from chunk {i+1} of {file_path.name} (length: {len(extracted_segment)}).")
                extracted_content_for_this_file_segments.append(extracted_segment.strip())
                
            elif extracted_segment == "":
                logger.debug(f"LLM returned empty content for chunk {i+1} of {file_path.name}. Not adding to final output.")
        
        if extracted_content_for_this_file_segments:
            
            if all_compiled_texts:
                all_compiled_texts.append(file_separator_text.strip())
            
            all_compiled_texts.append(f"--- CONTENT FROM: {file_path.name} ---")
            all_compiled_texts.append("\n\n".join(extracted_content_for_this_file_segments))
            
        else:
            logger.info(f"No content extracted from file: {file_path.name}")


    if not all_compiled_texts:
        logger.warning("No content was extracted from any file in Stage 5.")
        
        try:
            final_output_file_path.write_text("", encoding='utf-8')
            logger.info(f"Created empty output file at {final_output_file_path} as no content was extracted from any file.")
        
        except Exception as e:
            logger.error(f"Failed to create empty output file {final_output_file_path}: {e}", exc_info=True)
        
        return None

    logger.info("Compiling all extracted content into the final output file with text wrapping...")
    
    full_raw_text = "\n\n".join(all_compiled_texts)

    final_wrapped_text_lines = []
    for paragraph in full_raw_text.split("\n\n"):
        paragraph = paragraph.strip()
        if not paragraph:
            continue

        if paragraph.startswith("--- CONTENT FROM:") or paragraph == file_separator_text.strip():
            final_wrapped_text_lines.append(paragraph)
        
        else:
            wrapped_lines = textwrap.wrap(
                paragraph,
                width=text_wrap_width,
                replace_whitespace=True,
                drop_whitespace=True,
                break_long_words=False,
                fix_sentence_endings=True
            )
            
            final_wrapped_text_lines.extend(wrapped_lines)
            
        final_wrapped_text_lines.append("")

    final_output_content = "\n".join(final_wrapped_text_lines).strip()

    try:
        final_output_file_path.write_text(final_output_content, encoding='utf-8')
        logger.info(f"Successfully saved final compiled extracted content to {final_output_file_path} (Length: {len(final_output_content)})")
        return final_output_file_path
    
    except Exception as e:
        logger.error(f"Failed to save the final compiled output file {final_output_file_path}: {e}", exc_info=True)
        return None