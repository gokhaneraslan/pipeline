import logging
from typing import List, Optional, Any, Dict, Union
from pathlib import Path

from processing.document_parser import get_document_content, get_semantic_splitter, split_document_into_nodes
from processing.llm_interactor import llm_generate_questions_from_chunk, llm_answers_for_questions_from_chunk
from core.prompts import get_questions_generate_prompt, get_answered_questions_prompt
from core.utils import llm_create_template, llm_dataset_save_dataset

logger = logging.getLogger("DataPipeline")


def generate_questions_and_answers(
    input_final_content_file: Optional[Path],
    dataset_output_dir: Path,
    dataset_output_filename_base: str,
    num_questions_per_chunk: int,
    llm_dataset_template: str,
    llm_dataset_supported_templates: List[str],
    # SemanticSplitter config
    embedding_provider: str,
    ollama_embed_model: Optional[str],
    ollama_url: Optional[str],
    st_embed_model: Optional[str],
    semantic_splitter_buffer: int,
    semantic_splitter_breakpoint_perc: int,
    # LLM config for Q&A generation
    llm_client: Any,
    llm_provider_name: str,
    llm_qa_model_name: str,
    llm_temperature: float,
    llm_max_retries: int,
    llm_retry_delay: int ) -> Optional[str]:
    """
    Executes Stage 6: Takes the final compiled content file from Stage 5,
    splits it semantically, generates questions from each chunk,
    answers those questions based on the chunk, and saves the Q&A pairs.
    """
    
    stage_name = "Stage 6.1: Question & Answer Generation"
    
    logger.info(f"--- Starting {stage_name} ---")
    
    if not input_final_content_file or not input_final_content_file.exists():
        logger.warning(
            f"Input file from Stage 5 ('{input_final_content_file}') not provided or does not exist. "
            f"{stage_name} has no content to process."
        )
        return None
    
    logger.info(f"Input file (from Stage 5): {input_final_content_file.resolve()}")
    logger.info(f"Dataset output directory: {dataset_output_dir.resolve()}, Base Filename: {dataset_output_filename_base}")
    logger.info(f"Number of questions to generate per chunk: {num_questions_per_chunk}")
    logger.info(f"LLM Dataset Template: {llm_dataset_template}")

    dataset_output_dir.mkdir(parents=True, exist_ok=True)

    try:
        
        semantic_splitter = get_semantic_splitter(
            embedding_model_provider=embedding_provider,
            ollama_embedding_model_name=ollama_embed_model,
            ollama_base_url=ollama_url,
            sentence_transformers_model_name=st_embed_model,
            buffer_size=semantic_splitter_buffer,
            breakpoint_percentile_threshold=semantic_splitter_breakpoint_perc
        )
        
    except Exception as e:
        logger.critical(f"Failed to initialize SemanticSplitter for {stage_name}: {e}. Aborting.", exc_info=True)
        return None

    all_generated_questions: List[str] = []
    all_generated_answers: List[str] = []

    logger.info(f"Processing content from file: {input_final_content_file.name}")
    file_content = get_document_content(input_final_content_file)
    
    if file_content is None:
        logger.error(f"Could not read content from {input_final_content_file.name}. {stage_name} cannot proceed for this file.")
        return None
    
    if not file_content.strip():
        logger.warning(f"Content of {input_final_content_file.name} is empty. No Q&A will be generated from this file.")
        return None

    text_nodes = split_document_into_nodes(file_content, input_final_content_file.name, semantic_splitter)
    
    if not text_nodes:
        logger.warning(f"No text nodes/chunks generated by SemanticSplitter for {input_final_content_file.name}. No Q&A will be generated.")
        return None

    total_chunks = len(text_nodes)
    logger.info(f"Divided content into {total_chunks} chunks for Q&A generation.")

    for i, node in enumerate(text_nodes):
        chunk_text = node.get_content()
        logger.info(f"\nProcessing chunk {i+1}/{total_chunks} from {input_final_content_file.name}...")
        
        if not chunk_text.strip():
            logger.debug(f"Skipping empty chunk {i+1}/{total_chunks}.")
            continue

        questions_from_chunk: List[str] = llm_generate_questions_from_chunk(
            llm_client=llm_client,
            provider_name=llm_provider_name,
            model_name=llm_qa_model_name,
            chunk_text=chunk_text,
            num_questions=num_questions_per_chunk,
            temperature=llm_temperature,
            max_retries=llm_max_retries,
            retry_delay=llm_retry_delay,
            questions_generate_prompt_func=get_questions_generate_prompt
        )
        
        if not questions_from_chunk:
            logger.warning(f"No questions generated for chunk {i+1}/{total_chunks}. Skipping answer generation for this chunk.")
            continue
        
        logger.info(f"Generated {len(questions_from_chunk)} questions for chunk {i+1}.")

        answers_for_questions: List[str] = llm_answers_for_questions_from_chunk(
            llm_client=llm_client,
            provider_name=llm_provider_name,
            model_name=llm_qa_model_name,
            chunk_text=chunk_text,
            generated_questions=questions_from_chunk,
            temperature=llm_temperature,
            max_retries=llm_max_retries,
            retry_delay=llm_retry_delay,
            answered_questions_prompt_func=get_answered_questions_prompt
        )
            
        if len(answers_for_questions) == len(questions_from_chunk):
            
            all_generated_questions.extend(questions_from_chunk)
            all_generated_answers.extend(answers_for_questions)
            
            logger.info(f"Successfully generated {len(answers_for_questions)} answers for chunk {i+1}.")
            
        else:
            logger.warning(
                f"Mismatch in generated questions ({len(questions_from_chunk)}) and answers ({len(answers_for_questions)}) "
                f"for chunk {i+1}. This Q&A set will be skipped for this chunk."
            )

    if not all_generated_questions or not all_generated_answers:
        logger.warning(f"No valid Q&A pairs were generated from any chunk in {input_final_content_file.name}. Dataset will not be created.")
        return None
    
    logger.info(f"Total {len(all_generated_questions)} Q&A pairs generated from all chunks.")


    dataset_content_object: Union[List[Dict[str, str]], Dict[str, List[Dict[str, str]]], str] = llm_create_template(
        template_name=llm_dataset_template,
        generated_questions=all_generated_questions,
        answers_questions=all_generated_answers,
        supported_templates=llm_dataset_supported_templates
    )
    
    if isinstance(dataset_content_object, str):
        logger.error(f"Failed to create dataset template: {dataset_content_object}")
        return None
    
    saved_dataset_path = llm_dataset_save_dataset(
        dataset_data=dataset_content_object, 
        template_name=llm_dataset_template,
        dataset_output_base_path=str(dataset_output_dir / dataset_output_filename_base)
    )
    
    if saved_dataset_path:
        logger.info(f"Dataset generation completed. Saved to: {saved_dataset_path}")
        return saved_dataset_path
    
    else:
        logger.error("Failed to save the generated dataset.")
        return None