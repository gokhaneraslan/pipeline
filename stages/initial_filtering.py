import shutil
import logging
from pathlib import Path
from typing import List, Any, Dict, Optional

from processing.document_parser import get_document_content, get_sentence_splitter, split_document_into_nodes
from processing.llm_interactor import llm_get_score_for_chunk
from core.prompts import get_chunk_relevance_scoring_prompt


logger = logging.getLogger("DataPipeline")


def _calculate_average_chunk_score(
    file_path: Path,
    topic: str,
    sentence_splitter: Any,
    llm_client: Any,
    llm_provider_name: str,
    llm_scoring_model_name: str,
    llm_temperature: float,
    llm_max_retries: int,
    llm_retry_delay: int ) -> Optional[float]:
    """
    Calculates the average relevance score for all chunks in a given file.
    """
    
    logger.debug(f"Calculating average chunk RELEVANCE score for: {file_path.name}, topic: '{topic}'")
    
    file_content = get_document_content(file_path)
    
    if file_content is None:
        logger.error(f"Could not read content from {file_path.name}, cannot score for relevance.")
        return None
    
    if not file_content.strip():
        logger.warning(f"File {file_path.name} is empty or contains only whitespace. Assigning relevance score 0.0.")
        return 0.0


    text_nodes = split_document_into_nodes(file_content, file_path.name, sentence_splitter)
    
    if not text_nodes:
        logger.warning(f"No text nodes/chunks generated by SentenceSplitter for {file_path.name}, cannot calculate average relevance score.")
        return None


    scores: List[int] = []
    for i, node in enumerate(text_nodes):
        chunk_text = node.get_content()
        
        if not chunk_text.strip():
            logger.debug(f"Skipping empty chunk {i+1}/{len(text_nodes)} in {file_path.name} during relevance scoring.")
            continue
            
        score = llm_get_score_for_chunk(
            llm_client=llm_client,
            provider_name=llm_provider_name,
            model_name=llm_scoring_model_name,
            topic=topic,
            chunk_text=chunk_text,
            temperature=llm_temperature,
            max_retries=llm_max_retries,
            retry_delay=llm_retry_delay,
            scoring_prompt_func=get_chunk_relevance_scoring_prompt
        )
        
        if score is not None:
            logger.debug(f"Chunk {i+1}/{len(text_nodes)} of {file_path.name} (Relevance) scored: {score}")
            scores.append(score)
            
        else:
            logger.warning(f"Failed to get relevance score for chunk {i+1}/{len(text_nodes)} of {file_path.name}. Chunk will be ignored for average.")

    if not scores:
        logger.warning(f"No chunks were successfully relevance-scored for {file_path.name}. Cannot calculate average.")
        return None
    
    average_score = sum(scores) / len(scores)
    
    logger.info(f"File: {file_path.name} - Scored Chunks: {len(scores)}/{len(text_nodes)} - Average RELEVANCE Score: {average_score:.2f}")
    
    return average_score


def initial_file_filtering(
    input_files_dir: Path,
    output_relevant_files_dir: Path,
    topic: str,
    relevance_threshold: int,
    splitter_config: Dict[str, Any],
    llm_client: Any,
    llm_provider_name: str,
    llm_scoring_model_name: str,
    llm_temperature: float,
    llm_max_retries: int,
    llm_retry_delay: int) -> List[Path]:
    """
    Executes Stage 3: Filters files based on average chunk relevance score.
    """
    
    logger.info(f"--- Starting Stage 3: Initial File Filtering (Relevance Scoring) ---")
    logger.info(f"Input directory: {input_files_dir.resolve()}")
    logger.info(f"Output directory for relevant files: {output_relevant_files_dir.resolve()}")
    logger.info(f"Topic: '{topic}', Relevance Threshold: {relevance_threshold}")

    output_relevant_files_dir.mkdir(parents=True, exist_ok=True)

    files_to_process = [
        p for p in Path(input_files_dir).glob("*") 
        if p.is_file() and p.suffix.lower() in [".txt", ".pdf"]
    ]

    if not files_to_process:
        logger.warning(f"No .txt or .pdf files found in {input_files_dir}. Stage 3 has no files to process.")
        return []

    logger.info(f"Found {len(files_to_process)} files to process in {input_files_dir}.")

    sentence_splitter = get_sentence_splitter(
        chunk_size=splitter_config.get("chunk_size", 1024),
        chunk_overlap=splitter_config.get("chunk_overlap", 200),
    )

    relevant_file_paths: List[Path] = []
    for file_path in files_to_process:
        logger.info(f"\nProcessing file for relevance: {file_path.name}")
        
        average_score = _calculate_average_chunk_score(
            file_path=file_path,
            topic=topic,
            sentence_splitter=sentence_splitter,
            llm_client=llm_client,
            llm_provider_name=llm_provider_name,
            llm_scoring_model_name=llm_scoring_model_name,
            llm_temperature=llm_temperature,
            llm_max_retries=llm_max_retries,
            llm_retry_delay=llm_retry_delay
        )

        if average_score is not None:
            if average_score >= relevance_threshold:
                
                logger.info(f"File '{file_path.name}' marked RELEVANT (Avg Score: {average_score:.2f} >= {relevance_threshold})")
                
                destination_path = output_relevant_files_dir / file_path.name
                try:
                    shutil.copy2(file_path, destination_path)
                    logger.info(f"Copied '{file_path.name}' to '{destination_path}'")
                    relevant_file_paths.append(destination_path)
                
                except Exception as e:
                    logger.error(f"Failed to copy relevant file {file_path.name} to {destination_path}: {e}", exc_info=True)
            
            else:
                logger.info(f"File '{file_path.name}' marked NOT RELEVANT (Avg Score: {average_score:.2f} < {relevance_threshold})")
        
        else:
            logger.warning(f"Could not determine relevance for {file_path.name} (average score calculation failed). File will be SKIPPED (not marked relevant).")

    logger.info(f"--- Finished Stage 3: Initial File Filtering. Copied {len(relevant_file_paths)} relevant files. ---")
    
    return relevant_file_paths
